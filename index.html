<!DOCTYPE  html>
<html>
<head>

  <head>

  <style> 
    body {background-color:Beige;}
    h1 {color:Blue;}
  </style>
<img src="WebBanner2023.png" alt="Banner" width"900" height="300">
  <h1><b>ENG0018 Computor Labratory 2024/25</b></h1>
  <h2>Student URN: 6859142</h2>
  <hr> 
  <h2><b>Conference Paper : robot navigation of an environment using different visualisation sensors and softwares? </b></h2>
  <hr>
  <style>
    table  {
      font-family: arial, sans-serif;
      border-collapse: collapse;
      width: 30%;
    }
    td, th { 
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }
    tr:nth-child(even) {
      background-color: #dddddd;
    }
  </style>
  <style> 
  p.ex1{
    margin-left:250px;
  } </style>
  </head>

    

  <meta name= "viewport" content= " width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <style>
    .mySlides {display:none;}
  </style>
    <table>
    <tr>
      <th><h3>Table of Contents</h3></th>
    </tr>

    <td><a href="#Abstract">Abstract</td>
    <tr>
      <td><a href="#Introduction">Introduction</td>
    </tr>
      <tr>
         <td><a href="#LiDAR Technology">LiDAR Technology</td>
      </tr>
     <tr> <td><a href="#SLAM and VSLAM">SLAM and VSLAM</td></tr>
      
  </table>
<body>
<h3 id="Abstract">Abstract</h3>
<p class="ex1"> 
  <pre>
    Visualisation within robots is used to properly map out an environment they are positioned within, this can be performed by multiple methods such as Visual 
    Simultaneous Location and Mapping (VSLAM) (Visual SLAM: The Basics ,2020) and the use of Light Detection and ranging (LiDAR) technology (McManamon P, 2019). 
    These technologies have a variety of advantages and disadvantages to each and allow for further development through software’s which can advance these 
    technologies such as occlusion algorithms (Isobe Y, 2018) and computer vision algorithms e.g. YOLOv8 (Ultranlytics YOLOv8 , 2024) These technologies are also 
    synergetic and can work in unison to provide a highly accurate visualisation for the robot when it is placed in a dynamic environment. 
  </pre>
</p>
 <hr>
<h3 id="Introduction">Introduction</h3>
  <p class="ex1">
    <pre>
      Visualisation is a key part of a robot’s ability to be self-autonomous which allows for independent movement and analysis of an environment. This is done 
      through many aspects in robotics including producing a virtual environment for the robot to move in such as ones created in VSLAM while using measurement 
      techniques such as LIDAR which uses electromagnetic waves to provide accurate distances between the robot and objects in a 3D environment. These 
      measurements can allow for the robot to move more accurately in an environment and reduce the risk of damage to the robot from collisions. 
    </pre>
  <h3 id="LiDAR Technology">LiDAR Technology</h3>
  <p class="ex1">
    <pre>
      LiDAR is a technique used for tracking the distance between the sensor and an object in front of it by using the reflection of electromagnetic waves to pinpoint 
      displacement from the object (McManamon P, 2019). This array allows for visual confirmation of obstacles and depth as an obstacle and the wall behind it 
      would bounce back to the sensor at different speeds. This is like method used to estimate sea depth. With the equation: Distance = <Mfrac>(Time*Speed of light)/2</Mfrac> 
      This allows for an accurate depth of objects which the robot can visualise. This technique does not require a camera to extract information from images so 
      is more efficient than other visualization methods due to less data needing to be processed. However, this technique gives a visualization of the environment 
      which is difficult for an operator to read and act on. This shows that LiDAR is better suited for autonomous machines due to it retrieving data which is 
      difficult for a human operator to understand.
    </pre>
<h3 id="SLAM and VSLAM">SLAM and VSLAM</h3>
  <pre>
    SLAM “involves a system that simultaneously completes the positioning of the mobile robot itself and the map construction of the surrounding environment without 
    any prior environmental information” (Yang, S: SGC-VSLAM). This means that any sensor can be used such as LiDAR to interpret the environment and obstacles around 
    the robot. However, Visual SLAM differs from this due to the primary sensor for the robot being a camera. This is done through analysis of images and highlighting 
    key points within the image this can be seen in figure 1 below (Cai D et al, 2024) which shows the data which is extracted from photos when edge detection and 
    point detection are used alongside comparing two images to show motion detection. This demonstrates how a large scope of data can be retrieved using VSLAM and its 
    related software. However, there are some issues with this considering distortion from the camera lens which would impact the processing of key points. This is 
    solved through algorithms to undo this distortion shown in figure 2 below (Visual SLAM: The Basics, 2020) this is done using a distortion grid which makes the 
    image clearer and easier to produce key points and other estimations such as depth estimation. 
  </pre>
  
<div id="last-updated">Loading last update time...</div>
<!-- Verify Button -->
<button onclick="verifyLastUpdatedTime()" style="display: block; margin: 10px auto; padding: 8px 16px;">
    Verify Last Modified Time
</button>
<script>
    async function getLastUpdatedTime() {
        const username = 'FEPSFY6859142';
        const repo = 'OM6859142';
       
        const url = `https://api.github.com/repos/${username}/${repo}/commits`;
        try {
            const response = await fetch(url, {
                method: 'GET',
                headers: {
                    'Accept': 'application/vnd.github.v3+json',
                }
            });
            if (!response.ok) {
                throw new Error(`Error fetching data: ${response.status} - ${response.statusText}`);
            }
            const commits = await response.json();
            if (commits && commits.length > 0) {
                const lastCommitDate = new Date(commits[0].commit.committer.date);
               
                // Displaying the time on load
                document.getElementById('last-updated').innerText = `Last Modified Time: ${lastCommitDate.toLocaleString()}`;
            } else {
                document.getElementById('last-updated').innerText = 'No commits found in the repository.';
            }
        } catch (error) {
            console.error('Error fetching the last updated time:', error);
            document.getElementById('last-updated').innerText = 'Error fetching update time. Please check the repository details.';
        }
    }
    // Function to verify the last update time by re-fetching it from the API
    async function verifyLastUpdatedTime() {
        document.getElementById('last-updated').innerText = 'Verifying...';
        await getLastUpdatedTime();
        alert("Last modified time has been successfully verified from GitHub API.");
    }
    // Initial load to display the time on page load
    window.onload = getLastUpdatedTime;
</script>


 <!-- //////////////////////////////////////////////////////////////////////////////// -->
  <!-- ////////////////////////////// Word count function ////////////////////////////// -->
  <!-- //////////////////////////////////////////////////////////////////////////////// -->
<!-- Placeholder for total word count -->
<p id="totalWordCount"></p>
<hr>
<script>
  // Function to calculate and display word count for a specified section
  function displayWordCount(sectionId, outputId) {
    // Get the text content from the specified section
    const text = document.getElementById(sectionId).textContent;
    // Split text into words based on spaces and filter out any empty strings
    const wordArray = text.trim().split(/\s+/);
    // Count the words
    const wordCount = wordArray.length;
    // Return the word count for summing purposes
    return wordCount;
  }
  // Function to calculate and display total word count from selected sections
  function displayTotalWordCount() {
    // Calculate word count for each section and accumulate the total
    const IntroductionCount = displayWordCount("Introduction_InText");
    const AnalysisCount = displayWordCount("Analysis_InText");
    // Calculate the sum of all selected sections
    const totalWordCount = IntroductionCount + AnalysisCount;
    // Display the total word count
    document.getElementById("totalWordCount").innerText = `Total word count: ${totalWordCount}`;
  }
  // Run the function for specific sections and display total count when the page loads
  window.onload = displayTotalWordCount;
</script>

  </body>
</html>
