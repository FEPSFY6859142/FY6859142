<!DOCTYPE  html>
<html>
<head>

  <head>

  <style> 
    body {background-color:Beige;}
    h1 {color:Blue;}
  </style>
<img src="WebBanner2023.png" alt="Banner" width"900" height="300">
  <h1><b>ENG0018 Computor Labratory 2024/25</b></h1>
  <h2>Student URN: 6859142</h2>
  <hr> 
  <h2><b>Conference Paper : Using different visualisation software’s and sensors, how do robots navigate an environment?</b></h2>
  <hr>
  <style>
    table  {
      font-family: arial, sans-serif;
      border-collapse: collapse;
      width: 30%;
    }
    td, th { 
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }
    tr:nth-child(even) {
      background-color: #dddddd;
    }
  </style>
  <style> 
  p.ex1{
    margin-left:250px;
  } </style>
  </head>



  <meta name= "viewport" content= " width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <style>
    .mySlides {display:none;}
  </style>
  </head>
    <table>
    <tr>
      <th><h3>Table of Contents</h3></th>
    </tr>

    <td><a href="#Abstract">Abstract</td>
    <tr><td><a href="#Introduction">Introduction</td></tr>
      <tr><td><a href="#LiDAR">LiDAR Technology</td></tr>
     <tr> <td><a href="#SLAM">SLAM and VSLAM</td></tr>
      <tr> <td><a href="#Importance">Importance of 3D Visualisation</td></tr>
      <tr><td><a href="#Software">Software Applications</td></tr>
      <tr><td><a href="#Movement">Movement Tracking</td></tr>
      <tr><td><a href="#Conclusion">Conclusion</td></tr>
      <tr><td><a href="#References">References</td></tr>
  </table>
<body>
<h3 id="Abstract">Abstract</h3>
<p class="ex1"> 
  <pre>
    Visualisation within robots is used to properly map out an environment they are positioned within, this can be performed by multiple methods such as Visual 
    Simultaneous Location and Mapping (VSLAM) <a href = "https://www.kudan.io/blog/visual-slam-the-basics/">(Visual SLAM: The Basics ,2020)</a> and the use of Light Detection and ranging (LiDAR) technology (McManamon P, 2019). 
    These technologies have a variety of advantages and disadvantages to each and allow for further development through software’s which can advance these 
    technologies such as occlusion algorithms <a href= "https://robomechjournal.springeropen.com/articles/10.1186/s40648-018-0101-2">(Isobe Y et al, 2018)</a> and computer vision algorithms e.g. YOLOv8 <a href = "https://docs.ultralytics.com/models/yolov8/">(Ultranlytics YOLOv8 , 2024)</a> These technologies are also 
    synergetic and can work in unison to provide a highly accurate visualisation for the robot when it is placed in a dynamic environment. 
  </pre>
</p>
 <hr>
<h3 id="Introduction">Introduction</h3>
  <p class="ex1">
    <pre id="Introduction_Text">
      Visualisation is a key part of a robot’s ability to be self-autonomous which allows for independent movement and analysis of an environment. This is done 
      through many aspects in robotics including producing a virtual environment for the robot to move in such as ones created in VSLAM while using measurement 
      techniques such as LIDAR which uses electromagnetic waves to provide accurate distances between the robot and objects in a 3D environment. These 
      measurements can allow for the robot to move more accurately in an environment and reduce the risk of damage to the robot from collisions. 
    </pre>
  <h3 id="LiDAR">LiDAR Technology</h3>
  <p class="ex1">
    <pre id="LiDAR_Text">
      LiDAR is a technique used for tracking the distance between the sensor and an object in front of it by using the reflection of electromagnetic waves to pinpoint 
      displacement from the object <a href = "https://app-knovel-com.surrey.idm.oclc.org/kn/resources/kpLDARTS03/toc?cid=kpLDARTS03">(McManamon P, 2019)</a>. This array allows for visual confirmation of obstacles and depth as an obstacle and the wall behind it 
      would bounce back to the sensor at different speeds. This is like the method used to estimate sea depth. With the equation: Distance = <Mfrac>(Time*Speed of light)/2</Mfrac> 
      This allows for an accurate depth of objects which the robot can visualise. This technique does not require a camera to extract information from images so 
      is more efficient than other visualization methods due to less data needing to be processed. However, this technique gives a visualization of the environment 
      which is difficult for an operator to read and act on. This shows that LiDAR is better suited for autonomous machines due to it retrieving data which is 
      difficult for a human operator to understand.
    </pre>
</p>
<h3 id="SLAM">SLAM and VSLAM</h3>
  <p class="ex1">
  <pre id="SLAM_Text">
    SLAM “involves a system that simultaneously completes the positioning of the mobile robot itself and the map construction of the surrounding environment without 
    any prior environmental information” <a href = "https://doi.org/10.3390/s20082432">(Yang, S: SGC-VSLAM)</a>. This means that any sensor can be used such as LiDAR to interpret the environment and obstacles around 
    the robot. However, Visual SLAM differs from this due to the primary sensor for the robot being a camera. This is done through analysis of images and highlighting 
    key points within the image this can be seen in figure 1 below <a href = "https://link-springer-com.surrey.idm.oclc.org/article/10.1007/s10846-024-02171-7">(Cai D et al, 2024)</a> which shows the data which is extracted from photos when edge detection and 
    point detection are used alongside comparing two images to show motion detection. This demonstrates how a large scope of data can be retrieved using VSLAM and its 
    related software. However, there are some issues with this considering distortion from the camera lens which would impact the processing of key points. This is 
    solved through algorithms to undo this distortion shown in figure 2 below <a href = "https://www.kudan.io/blog/visual-slam-the-basics/">(Visual SLAM: The Basics, 2020)</a> this is done using a distortion grid which makes the 
    image clearer and easier to produce key points and other estimations such as depth estimation. 
  </pre>
</p>
  <img src="KeyPoint.png" alt="Figure 1" width"800" height="500">
  <h4>Figure 1</h4>
  <img src="Distortion.png" alt="Figure 2" width "800" Height = "500">
  <h4>Figure 2</h4>
  <h3 id="Importance">Importance of 3D Visualisation</h3>
<p class="ex1">
  <pre id= "Importance_Text">
    3d visualization is key to allowing for robots to act autonomously due to it allowing for the robot to process its environment around it without the aid of an 
    operator. Diversity in how an environment is read allows for more accurate readings of an environment and will remove any error which could be caused by faulty 
    sensors or inaccuracies in its programming. An example of where 3d visualisation is important is the ROBONAUT 2<a href = "https://www.nasa.gov/missions/station/nasa-space-robotics-dive-into-deep-sea-work/">(NASA , 2022)</a>.  This shows that visualisation software like 
    this is key when a human is unable to see in an environment. Visualisation software is key to the navigation of unknown environments where humans cannot 
    travel and in which there needs to be a response where human involvement is dangerous or impossible for a human to achieve. 
  </pre>
</p>
  <h3 id="Software">Software Applications</h3>
<p class="ex1">
  <pre id = "Software_Text">
    Sensors are a single part of the process of 3d Visualisation. The most important part is the algorithms and processes such as YOLO-8 <a href = "https://docs.ultralytics.com/models/yolov8/ ">(Ultranlytics YOLOv8, 2024)</a>
    which is a software that allows for computer vision and can be specified to unique purposes based on what the robot requires for instance a Rubik’s cube solver 
    would require an orientation-based installation of YOLO to determine the position of people. This links with the discussed visualisation methods as it can 
    process the information that VSLAM receives from the environment. Other software such as occlusion algorithms shown in a paper by <a href = "https://robomechjournal.springeropen.com/articles/10.1186/s40648-018-0101-2">Yuzuka Isobe (2018)</a> Which 
    details how occlusion is used to negate occlusion due to light changes this allows for objects to be seen when in darkness the paper also discusses that 
    Machine Learning is used for occlusion algorithms using predefined data sets which can be analysed to teach the program what should exist in the occluded area.
  </pre>
</p>
  <h3 id="Movement">Movement Tracking</h3>
  <p class = "ex1">
    <pre id="Movement_Text">
      A key part of a robot is dealing with position tracking this can be done through multiple means such as an odometer which tracks the number of wheel 
      rotations to estimate a distance travelled from the origin which will allow for the robot to be tracked. This however is not accurate and may mean that the 
      robots perceived location in its virtual environment is different than its physical location. This can be solved by using multiple methods of tracking the
      robot’s movement another method of this is by calculating the average speed of the robot which will determine its net movement from its starting position. 
      This allows for data to be reinforced and will provide a more accurate position of the robot.
    </pre>
</p>
  <h3 id="Conclusion">Conclusion</h3>
  <p class="ex1">
    <pre id="Conclusion_Text">
      Robot visualization requires all these different software’s and sensors to provide an accurate analysis of the robot’s environment. This is because 
      positioning the robot correctly is essential to it functioning. This is aided through multiple inputs from both cameras where SLAM technology can be used 
      and LiDAR to give a 3d view of obstacles. Software such as YOLO is also key in simplifying the processes by which the code understands its environment 
      while using occlusion to predict the position of obstacles before it can be written.
    </pre>
  </p>

  <h3 id="References">References</h3>
  <p class= "ex1">
    <pre>
<a href = "https://link-springer-com.surrey.idm.oclc.org/article/10.1007/s10846-024-02171-7">Cai D, Li S , Qi W, Ding K , Lu J , Lui G , Hu Z (2024) DFT VSLAM: A Dynamic Optical Flow Tracking VSLAM Method Available at: https://link-springer-com.surrey.idm.oclc.org/article/
  10.1007/s10846-024-02171-7</a>
      
<a href = "https://doi.org/10.1007/s11370-019-00283-w">Huang, K., Chitrakar, D., Rydén, F., & Chizeck, H. J. (2019). Evaluation of haptic guidance virtual fixtures and 3D visualization methods in telemanipulation—a user study. 
  Intelligent Service Robotics, 12(4), 289–301.  </a>  
      
<a href = "https://robomechjournal.springeropen.com/articles/10.1186/s40648-018-0101-2">Isobe Y, Masuyama G & Umeda K. (2018) Occlusion handling for a target-tracking robot with a stereo camera. Available at: https://robomechjournal.springeropen.com/articles/10.1186/
  s40648-018-0101-2</a>
  
<a href = "https://doi.org/10.1109/IROS51168.2021.9635947">Li, M., Weber, C., Kerzel, M., Lee, J. H., Zeng, Z., Liu, Z., & Wermter, S. (2021). Robotic Occlusion Reasoning for Efficient Object Existence Prediction. 2021 IEEE/RSJ International 
  Conference on Intelligent Robots and Systems (IROS), 2686–2692. https://doi.org/10.1109/IROS51168.2021.9635947 </a>
  
<a href = "https://app-knovel-com.surrey.idm.oclc.org/kn/resources/kpLDARTS03/toc?cid=kpLDARTS03">McManamon, P. (2019) LiDAR Technologies and systems. Available at: https://app-knovel-com.surrey.idm.oclc.org/kn/resources/kpLDARTS03/toc?cid=kpLDARTS03</a>

<a href = "https://www.nasa.gov/missions/station/nasa-space-robotics-dive-into-deep-sea-work/">NASA Space Robotics Dive into Deep-Sea Work(2022,August 2). NASA. https://www.nasa.gov/missions/station/nasa-space-robotics-dive-into-deep-sea-work/</a>

<a href = "https://docs.ultralytics.com/models/yolov8/ ">Ultranlytics YOLOv8 (2024) Available at: https://docs.ultralytics.com/models/yolov8/ Accessed at: (1st November 2024)</a>

<a href = "https://www.kudan.io/blog/visual-slam-the-basics/">Visual SLAM: The Basics (2020) Available at: https://www.kudan.io/blog/visual-slam-the-basics/ (Accessed:1st November 2024)</a> 

<a href = "https://doi.org/10.3390/s20082432">Yang, S., Fan, G., Bai, L., Zhao, C., & Li, D. (2020). SGC-VSLAM: A Semantic and Geometric Constraints VSLAM for Dynamic Indoor Environments. 
  Available at:  https://doi.org/10.3390/s20082432</a>    
    </pre>
  
</p>
<!-- //////////////////////////////////////////////////////////////////////////////// -->
  <!-- ////////////////////////////// Adding last update ////////////////////////////// -->
  <!-- //////////////////////////////////////////////////////////////////////////////// -->
    <!-- Last commit time display -->
<div id="last-updated">Loading last update time...</div>
<!-- Verify Button -->
<button onclick="verifyLastUpdatedTime()" style="display: block; margin: 10px auto; padding: 8px 16px;">
    Verify Last Modified Time
</button>
<script>
    async function getLastUpdatedTime() {
        const username = 'FEPSFY6859142';
        const repo = 'OM6859142';
       
        const url = `https://api.github.com/repos/${username}/${repo}/commits`;
        try {
            const response = await fetch(url, {
                method: 'GET',
                headers: {
                    'Accept': 'application/vnd.github.v3+json',
                }
            });
            if (!response.ok) {
                throw new Error(`Error fetching data: ${response.status} - ${response.statusText}`);
            }
            const commits = await response.json();
            if (commits && commits.length > 0) {
                const lastCommitDate = new Date(commits[0].commit.committer.date);
               
                // Displaying the time on load
                document.getElementById('last-updated').innerText = `Last Modified Time: ${lastCommitDate.toLocaleString()}`;
            } else {
                document.getElementById('last-updated').innerText = 'No commits found in the repository.';
            }
        } catch (error) {
            console.error('Error fetching the last updated time:', error);
            document.getElementById('last-updated').innerText = 'Error fetching update time. Please check the repository details.';
        }
    }
    // Function to verify the last update time by re-fetching it from the API
    async function verifyLastUpdatedTime() {
        document.getElementById('last-updated').innerText = 'Verifying...';
        await getLastUpdatedTime();
        alert("Last modified time has been successfully verified from GitHub API.");
    }
    // Initial load to display the time on page load
    window.onload = getLastUpdatedTime;
</script>


 <!-- //////////////////////////////////////////////////////////////////////////////// -->
  <!-- ////////////////////////////// Word count function ////////////////////////////// -->
  <!-- //////////////////////////////////////////////////////////////////////////////// -->
<!-- Placeholder for total word count -->
<p id="totalWordCount"></p>
<hr>
<script>
  // Function to calculate and display word count for a specified section
  function displayWordCount(sectionId, outputId) {
    // Get the text content from the specified section
    const text = document.getElementById(sectionId).textContent;
    // Split text into words based on spaces and filter out any empty strings
    const wordArray = text.trim().split(/\s+/);
    // Count the words
    const wordCount = wordArray.length;
    // Return the word count for summing purposes
    return wordCount;
  }
  // Function to calculate and display total word count from selected sections
  function displayTotalWordCount() {
    // Calculate word count for each section and accumulate the total
    const IntroductionCount = displayWordCount("Introduction_Text");
    const LiDARCount = displayWordCount("LiDAR_Text");
    const SLAMCount = displayWordCount("SLAM_Text");
    const ImportanceCount = displayWordCount("Importance_Text");
    const SoftwareCount = displayWordCount("Software_Text");
    const MovementCount = displayWordCount("Movement_Text");
    const ConclusionCount = displayWordCount("Conclusion_Text");
    // Calculate the sum of all selected sections
    const totalWordCount = IntroductionCount + LiDARCount + SLAMCount + ImportanceCount + SoftwareCount + MovementCount + ConclusionCount;
    // Display the total word count
    document.getElementById("totalWordCount").innerText = `Total word count: ${totalWordCount}`;
  }
  // Run the function for specific sections and display total count when the page loads
  window.onload = displayTotalWordCount;
</script>



  </body>
</html>
